{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.stem.arlstem import ARLSTem\n",
    "from nltk.classify import SklearnClassifier\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "\n",
    "#from wordcloud import WordCloud,STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['لينكبرايف', 'كلمهمنشن', 'كلمهشتاج', 'كلمهموجب', 'كلمهسالب']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_lex_file_path = '../data/pos_lex.txt'\n",
    "negative_lex_file_path = '../data/neg_lex.txt' \n",
    "stop_words_file_path = '../data/arabicStops.txt' \n",
    "\n",
    "link_word = \"لينكبرايف\"\n",
    "mention_word = \"كلمهمنشن\"\n",
    "hashtag_word = \"كلمهشتاج\"\n",
    "positive_word = \"كلمهموجب\"\n",
    "negative_word = \"كلمهسالب\"\n",
    "\n",
    "preserved_words = [link_word,mention_word,hashtag_word,positive_word,negative_word]\n",
    "preserved_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_elongation(text):\n",
    "    a=re.sub(r\"ا+\", \"ا\", text)\n",
    "    b=re.sub(r\"ب+\", \"ب\", a)\n",
    "    c=re.sub(r\"ت+\", \"ت\", b)\n",
    "    d=re.sub(r\"ث+\", \"ث\", c)\n",
    "    e=re.sub(r\"ج+\", \"ج\", d)\n",
    "    f=re.sub(r\"ح+\", \"ح\", e)\n",
    "    g=re.sub(r\"خ+\", \"خ\", f)\n",
    "    q=re.sub(r\"د+\", \"د\", g)\n",
    "    w=re.sub(r\"ذ+\", \"ذ\", q)\n",
    "    r=re.sub(r\"ش+\", \"ش\", w)\n",
    "    t=re.sub(r\"س+\", \"س\", r)\n",
    "    y=re.sub(r\"ط+\", \"ط\", t)\n",
    "    u=re.sub(r\"ع+\", \"ع\", y)\n",
    "    i=re.sub(r\"غ+\", \"غ\", u)\n",
    "    o=re.sub(r\"م+\", \"م\", i)\n",
    "    p=re.sub(r\"ه+\", \"ه\", o)\n",
    "    s=re.sub(r\"و+\", \"و\", p)\n",
    "    j=re.sub(r\"ي+\", \"ي\", s)\n",
    "    k=re.sub(r\"ى+\", \"ى\", j)\n",
    "    l=re.sub(r\"ر+\", \"ر\", k)\n",
    "    x=re.sub(r\"ز+\", \"ز\", l)\n",
    "    x1 = re.sub(r\"ن+\", \"ن\", x)\n",
    "    x2 = re.sub(r\"ف+\", \"ف\", x1)\n",
    "    x3 = re.sub(r\"ئ+\", \"ئ\", x2)\n",
    "    return x3\n",
    "    \n",
    "def normalization(text):\n",
    "    a=re.sub(r\"أ\", \"ا\", text)\n",
    "    b=re.sub(r\"اْ\", \"ا\", a)\n",
    "    s = re.sub(r\"آ\", \"ا\", b)\n",
    "    return s\n",
    "\n",
    "def remove_number(text):\n",
    "    a=re.sub(r\"\\d\", \"\", text)\n",
    "    return a\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    a=re.sub(r\"[!_|<>;:۔.``..$?]\",\"\", text)\n",
    "    return a\n",
    "\n",
    "def read_stop_words_from_file(file_path, encoding='utf-8'):\n",
    "    stop_words_file = open(file_path, encoding=encoding)\n",
    "    arabic_stop_words = stop_words_file.read()\n",
    "    arabic_stop_words = [w.strip() for w in arabic_stop_words.split('\\n')]\n",
    "    return list(set(arabic_stop_words))\n",
    "\n",
    "def read_pos_lex(file_path, encoding='utf-8'):\n",
    "    pos_lex_file = open(file_path, encoding=encoding)\n",
    "    pos_lex = pos_lex_file.read()\n",
    "    pos_lex = [w.strip() for w in pos_lex.split('\\n')]\n",
    "    return list(set(pos_lex))\n",
    "\n",
    "def read_neg_lex(file_path, encoding='utf-8'):\n",
    "    neg_lex_file = open(file_path, encoding=encoding)\n",
    "    neg_lex = neg_lex_file.read()\n",
    "    neg_lex = [w.strip() for w in neg_lex.split('\\n')]\n",
    "    return list(set(neg_lex))\n",
    "\n",
    "def match_preserved_word(word):\n",
    "    for w in preserved_words:\n",
    "        if w in word:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def custom_stop_words(file_path):\n",
    "    custom_stop_words = read_stop_words_from_file(file_path)\n",
    "    stop_list1 = list(set(stopwords.words(\"Arabic\")))\n",
    "    all_stop_words = list(set(stop_list1 + custom_stop_words))\n",
    "    return all_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(text):\n",
    "    search = [\"أ\", \"إ\", \"آ\", \"ة\", \"_\", \"-\", \"/\", \".\", \"،\", \" و \", \" يا \", '\"', \"ـ\", \"'\", \"ى\", \"\\\\\", '\\n', '\\t',\n",
    "              '&quot;', '?', '؟', '!']\n",
    "    replace = [\"ا\", \"ا\", \"ا\", \"ه\", \" \", \" \", \"\", \"\", \"\", \" و\", \" يا\", \"\", \"\", \"\", \"ي\", \"\", ' ', ' ', ' ', ' ? ', ' ؟ ',\n",
    "               ' ! ']\n",
    "\n",
    "    # remove tashkeel\n",
    "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
    "    text = re.sub(p_tashkeel, \"\", text)\n",
    "\n",
    "    # remove longation\n",
    "    p_longation = re.compile(r'(.)\\1+')\n",
    "    subst = r\"\\1\\1\"\n",
    "    text = re.sub(p_longation, subst, text)\n",
    "\n",
    "    text = text.replace('وو', 'و')\n",
    "    text = text.replace('يي', 'ي')\n",
    "    text = text.replace('اا', 'ا')\n",
    "\n",
    "    for i in range(0, len(search)):\n",
    "        text = text.replace(search[i], replace[i])\n",
    "\n",
    "    # trim\n",
    "    #text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_sentence(text,\n",
    "                   positive_lex = read_pos_lex(positive_lex_file_path),\n",
    "                   negative_lex = read_neg_lex(negative_lex_file_path),\n",
    "                   stemmer= ARLSTem(),\n",
    "                   stopwords = custom_stop_words(stop_words_file_path)):\n",
    "    \n",
    "    text = clean_str(text)\n",
    "    \n",
    "    cleaned_sentence = re.sub(r\"http\\S+\", link_word, text)  # link remove\n",
    "    \n",
    "    cleaned_sentence = re.sub(r\"@\\S+\", mention_word, cleaned_sentence)#mention_replace\n",
    "    \n",
    "    cleaned_sentence = re.sub(r\"#\\S+\", hashtag_word, cleaned_sentence)#hastag_replacer\n",
    "    \n",
    "    cleaned_sentence = re.sub(r\"[A-Z]\", \"\", cleaned_sentence)#remove_capital\n",
    "    \n",
    "    cleaned_sentence = re.sub(r\"[a-z]\", \"\", cleaned_sentence)#remove_small\n",
    "    \n",
    "    stemmed_sentence = stemmer.stem(cleaned_sentence)  # stemmer\n",
    "    \n",
    "    final = stemmer.norm(remove_number(remove_punctuations(normalization(remove_elongation(stemmed_sentence)))))#removeelnogation #removenuber #normalize_word #remove punc\n",
    "    \n",
    "    final = word_tokenize(final) #word_tokenize\n",
    "    \n",
    "    output = [w for w in final if not w in stopwords]\n",
    "    \n",
    "    pos_postfix = [positive_word for w in output if w in positive_lex]\n",
    "    neg_postfix = [negative_word for w in output if w in negative_lex]\n",
    "    \n",
    "    #pos_words = [w for w in output if w in positive_lex]\n",
    "    #neg_words = [w for w in output if w in negative_lex]\n",
    "    #print(pos_words)\n",
    "    #print(neg_words)\n",
    "    \n",
    "    output += pos_postfix \n",
    "    output += neg_postfix\n",
    "    \n",
    "    textOnly = [w for w in output if match_preserved_word(w) == False]\n",
    "    \n",
    "    return ' '.join(output), ' '.join(textOnly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize(text, tfidf_model, feature_len = 5):\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    perc_link =  (len(re.findall(r\"لينكبرايف\",text)))/feature_len\n",
    "    perc_mention = (len(re.findall(r\"كلمهمنشن\", text)))/feature_len  \n",
    "    perc_hash = (len(re.findall(r\"كلمهشتاج\", text)))/feature_len  \n",
    "    perc_neg = (len(re.findall(r\"كلمهسالب\", text)))/feature_len\n",
    "    perc_pos = (len(re.findall(r\"كلمهموجب\", text)))/feature_len\n",
    "    \n",
    "    if perc_link >= 1: perc_link = 0.99\n",
    "    if perc_mention >= 1: perc_mention = 0.99\n",
    "    if perc_hash >= 1: perc_hash = 0.99\n",
    "    if perc_neg >= 1: perc_neg = 0.99\n",
    "    if perc_pos >= 1: perc_pos = 0.99\n",
    "        \n",
    "    lenght = len(text)\n",
    "    if lenght <= 10:\n",
    "        rs = [1, 0, 0]\n",
    "    elif lenght <= 20:\n",
    "        rs = [0, 1, 0]\n",
    "    else:\n",
    "        rs = [0, 0, 1]\n",
    "\n",
    "    features_vec=[perc_link,perc_mention,perc_hash,perc_neg,perc_pos]\n",
    "    features_vec += rs\n",
    "    \n",
    "    tfidf_feature = tfidf_model.transform(np.array([text])).toarray()\n",
    "\n",
    "    return features_vec , list(tfidf_feature[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>negative</td>\n",
       "      <td>#3alahwa كان عندنا مدرس بيضربنا بجلدة حمير واح...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>positive</td>\n",
       "      <td>@malak_alhusainiاختي ملاك امي معاها ايفون ٤ ود...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5725</th>\n",
       "      <td>neutral</td>\n",
       "      <td>آللّهُمَّ صّلِ وسَلّمْ عَلۓِ سَيّدنَآ مُحَمد ﷺ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7987</th>\n",
       "      <td>neutral</td>\n",
       "      <td>' #MinaAtta_on_cbc مينا عطا  الكل قدام الtv وك...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>negative</td>\n",
       "      <td>@YmnNow لا يا خوان داعش اين المرتبات ،،يالصوص ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         class                                              tweet\n",
       "3990  negative  #3alahwa كان عندنا مدرس بيضربنا بجلدة حمير واح...\n",
       "1120  positive  @malak_alhusainiاختي ملاك امي معاها ايفون ٤ ود...\n",
       "5725   neutral     آللّهُمَّ صّلِ وسَلّمْ عَلۓِ سَيّدنَآ مُحَمد ﷺ\n",
       "7987   neutral  ' #MinaAtta_on_cbc مينا عطا  الكل قدام الtv وك...\n",
       "2062  negative  @YmnNow لا يا خوان داعش اين المرتبات ،،يالصوص ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/datasets/combined_dataset.csv', encoding = \"utf-8\")\n",
    "data = data[['class','tweet']]\n",
    "data = data.reindex(np.random.permutation(data.index))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "آللّهُمَّ صّلِ وسَلّمْ عَلۓِ سَيّدنَآ مُحَمد ﷺ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('صل وسلم علۓ سيدنا محمد ﷺ', 'صل وسلم علۓ سيدنا محمد ﷺ')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = data.iloc[2]['tweet']\n",
    "print(test)\n",
    "out_test = clean_sentence(test)\n",
    "out_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['cleaned_tweet'] = data['tweet'].map(lambda v: clean_sentence(v)[0])\n",
    "data['cleaned_tweet_text_only'] = data['tweet'].map(lambda v: clean_sentence(v)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>cleaned_tweet_text_only</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>negative</td>\n",
       "      <td>#3alahwa كان عندنا مدرس بيضربنا بجلدة حمير واح...</td>\n",
       "      <td>كلمهشتاج عندنا مدرس بيضربنا بجلده حمير واحنا ك...</td>\n",
       "      <td>عندنا مدرس بيضربنا بجلده حمير واحنا كنا نقعد ن...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>positive</td>\n",
       "      <td>@malak_alhusainiاختي ملاك امي معاها ايفون ٤ ود...</td>\n",
       "      <td>كلمهمنشن اختي ملاك امي معاها ايفون ودي اشتري ا...</td>\n",
       "      <td>اختي ملاك امي معاها ايفون ودي اشتري ايفون ودي ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5725</th>\n",
       "      <td>neutral</td>\n",
       "      <td>آللّهُمَّ صّلِ وسَلّمْ عَلۓِ سَيّدنَآ مُحَمد ﷺ</td>\n",
       "      <td>صل وسلم علۓ سيدنا محمد ﷺ</td>\n",
       "      <td>صل وسلم علۓ سيدنا محمد ﷺ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7987</th>\n",
       "      <td>neutral</td>\n",
       "      <td>' #MinaAtta_on_cbc مينا عطا  الكل قدام الtv وك...</td>\n",
       "      <td>كلمهشتاج مينا عطا الكل قدام وكانه مباراه المنت...</td>\n",
       "      <td>مينا عطا الكل قدام وكانه مباراه المنتخب نهائي ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>negative</td>\n",
       "      <td>@YmnNow لا يا خوان داعش اين المرتبات ،،يالصوص ...</td>\n",
       "      <td>كلمهمنشن ياخوان داعش المرتبات يالصوص العصر</td>\n",
       "      <td>ياخوان داعش المرتبات يالصوص العصر</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         class                                              tweet  \\\n",
       "3990  negative  #3alahwa كان عندنا مدرس بيضربنا بجلدة حمير واح...   \n",
       "1120  positive  @malak_alhusainiاختي ملاك امي معاها ايفون ٤ ود...   \n",
       "5725   neutral     آللّهُمَّ صّلِ وسَلّمْ عَلۓِ سَيّدنَآ مُحَمد ﷺ   \n",
       "7987   neutral  ' #MinaAtta_on_cbc مينا عطا  الكل قدام الtv وك...   \n",
       "2062  negative  @YmnNow لا يا خوان داعش اين المرتبات ،،يالصوص ...   \n",
       "\n",
       "                                          cleaned_tweet  \\\n",
       "3990  كلمهشتاج عندنا مدرس بيضربنا بجلده حمير واحنا ك...   \n",
       "1120  كلمهمنشن اختي ملاك امي معاها ايفون ودي اشتري ا...   \n",
       "5725                           صل وسلم علۓ سيدنا محمد ﷺ   \n",
       "7987  كلمهشتاج مينا عطا الكل قدام وكانه مباراه المنت...   \n",
       "2062         كلمهمنشن ياخوان داعش المرتبات يالصوص العصر   \n",
       "\n",
       "                                cleaned_tweet_text_only  \n",
       "3990  عندنا مدرس بيضربنا بجلده حمير واحنا كنا نقعد ن...  \n",
       "1120  اختي ملاك امي معاها ايفون ودي اشتري ايفون ودي ...  \n",
       "5725                           صل وسلم علۓ سيدنا محمد ﷺ  \n",
       "7987  مينا عطا الكل قدام وكانه مباراه المنتخب نهائي ...  \n",
       "2062                  ياخوان داعش المرتبات يالصوص العصر  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120    @malak_alhusainiاختي ملاك امي معاها ايفون ٤ ود...\n",
      "5725       آللّهُمَّ صّلِ وسَلّمْ عَلۓِ سَيّدنَآ مُحَمد ﷺ\n",
      "Name: tweet, dtype: object\n",
      "صل وسلم علۓ سيدنا محمد ﷺ\n",
      "صل وسلم علۓ سيدنا محمد ﷺ\n"
     ]
    }
   ],
   "source": [
    "print(data.iloc[1:3]['tweet'])\n",
    "print(data.iloc[2]['cleaned_tweet'])\n",
    "print(data.iloc[2]['cleaned_tweet_text_only'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "featurize() missing 1 required positional argument: 'tfidf_model'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-a0fc0dc225c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeaturize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m102\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cleaned_tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: featurize() missing 1 required positional argument: 'tfidf_model'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "featurize(data.iloc[1:102]['cleaned_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_document = data['cleaned_tweet']\n",
    "#print(all_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=word_tokenize)\n",
    "tfidf_model = sklearn_tfidf.fit(all_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), tokenizer=word_tokenize, min_df=1)\n",
    "bigramModel = bigram_vectorizer.fit(all_document)\n",
    "c2 = bigramModel.transform(all_document).toarray()\n",
    "c2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramModel.transform(np.array([\"this is a test sent\"])).toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = tfidf_model.transform(all_document[1:2]).toarray()\n",
    "c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = featurize(all_document[4:5], tfidf_model=tfidfmodel)\n",
    "#np.array([out]).shape\n",
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}